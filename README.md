# GeliÅŸmiÅŸ RAG Agent Projesi (Hybrid Search & Contextualization)

Bu proje, yÃ¼klenen PDF belgeleri Ã¼zerinden sorularÄ± yanÄ±tlamak iÃ§in tasarlanmÄ±ÅŸ geliÅŸmiÅŸ bir Retrieval-Augmented Generation (RAG) sistemidir. Standart RAG yaklaÅŸÄ±mlarÄ±nÄ±n Ã¶tesine geÃ§erek, arama isabetliliÄŸini artÄ±rmak iÃ§in **Hibrit Arama (Hybrid Search)**, **Anlamsal ParÃ§alama (Semantic Chunking)** ve **LLM ile Otomatik BaÄŸlamsallaÅŸtÄ±rma (Contextualization)** gibi modern teknikleri kullanÄ±r.

Sistem, bir LangChain Agent'Ä± ve Gradio arayÃ¼zÃ¼ ile paketlenmiÅŸtir, bu sayede kullanÄ±cÄ±lar hem sohbet edebilir hem de dinamik olarak yeni belgeler yÃ¼kleyip indeksi gÃ¼ncelleyebilir.

## ğŸš€ Temel Ã–zellikler

* **Hibrit Arama (Hybrid Search):** Hem anahtar kelime tabanlÄ± (Sparse, TF-IDF) hem de anlamsal (Dense, SBERT) aramayÄ± birleÅŸtirerek her iki dÃ¼nyanÄ±n da avantajlarÄ±nÄ± kullanÄ±r. SonuÃ§lar normalleÅŸtirilir ve aÄŸÄ±rlÄ±klÄ± bir skorla birleÅŸtirilir.
* **Anlamsal ParÃ§alama (Semantic Chunking):** Metinleri sabit boyutlu parÃ§alara bÃ¶lmek yerine, anlamsal olarak iliÅŸkili cÃ¼mleleri (cÃ¼mleler arasÄ± kosinÃ¼s benzerliÄŸine gÃ¶re) bir arada tutan bir `hybrid_chunker` kullanÄ±r.
* **LLM ile BaÄŸlamsallaÅŸtÄ±rma:** Ä°ndekslemeden *Ã¶nce*, her bir metin parÃ§asÄ±nÄ±n (chunk) belge iÃ§indeki yerini daha iyi aÃ§Ä±klamasÄ± iÃ§in kÃ¼Ã§Ã¼k bir LLM (`meta-llama/Llama-3.2-1B-Instruct`) kullanarak Ã¶zet bir baÄŸlam (context) Ã¼retilir ve bu, parÃ§anÄ±n baÅŸÄ±na eklenir. Bu, arama sÄ±rasÄ±nda alaka dÃ¼zeyini Ã¶nemli Ã¶lÃ§Ã¼de artÄ±rÄ±r.
* **AkÄ±llÄ± Agent:** SorgularÄ± iÅŸlemek ve `DocumentHybridSearch` aracÄ±nÄ± akÄ±llÄ±ca kullanmak iÃ§in `ChatGoogleGenerativeAI` (Gemini) modeli ile gÃ¼Ã§lendirilmiÅŸ bir LangChain agent'Ä± iÃ§erir.
* **Dinamik Ä°ndeksleme:** Gradio arayÃ¼zÃ¼ Ã¼zerinden yeni PDF'ler yÃ¼klendiÄŸinde tÃ¼m veri iÅŸleme (ingestion) pijplini (`run_rebuild`) otomatik olarak tetiklenir ve agent hafÄ±zada gÃ¼ncellenir.

## âš™ï¸ Mimari ve Veri AkÄ±ÅŸÄ±

Proje, iki ana aÅŸamadan oluÅŸur: **Veri Ä°ÅŸleme (Ingestion)** ve **Sorgulama (Inference)**.

### 1. Veri Ä°ÅŸleme (Ingestion) Pijplini
Yeni bir PDF yÃ¼klendiÄŸinde (`app.py` -> `upload_and_reindex` -> `run_rebuild`):

1.  **PDF AyrÄ±ÅŸtÄ±rma:** PDF, `PyMuPDF` (fitz) kullanÄ±larak baÅŸlÄ±klarÄ±na (font kalÄ±nlÄ±ÄŸÄ± ve boÅŸluklara gÃ¶re) ayrÄ±ÅŸtÄ±rÄ±lÄ±r ve `(baÅŸlÄ±k, iÃ§erik)` Ã§iftleri olarak kaydedilir.
2.  **Anlamsal ParÃ§alama:** Her bÃ¶lÃ¼mÃ¼n iÃ§eriÄŸi, `hybrid_chunker` ile anlamsal olarak tutarlÄ± parÃ§alara (chunks) bÃ¶lÃ¼nÃ¼r.
3.  **BaÄŸlamsallaÅŸtÄ±rma:** Her parÃ§a, `CONTEXT_MODEL_NAME` (`Llama-3.2-1B`) modeline gÃ¶nderilerek bir "baÄŸlam" Ã¶zeti Ã¼retilir ve bu Ã¶zet parÃ§anÄ±n baÅŸÄ±na eklenir (`context + chunk_text`).
4.  **VektÃ¶rleÅŸtirme (Dense):** BaÄŸlamsallaÅŸtÄ±rÄ±lmÄ±ÅŸ parÃ§alar `EMBED_MODEL_NAME` (`all-MiniLM-L6-v2`) ile gÃ¶mme (embedding) vektÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.
5.  **VektÃ¶rleÅŸtirme (Sparse):** TÃ¼m parÃ§alar Ã¼zerinde bir `TfidfVectorizer` eÄŸitilir (`vectorizer.joblib` olarak kaydedilir) ve sparse vektÃ¶rler oluÅŸturulur.
6.  **Ä°ndeksleme:** Dense ve Sparse vektÃ¶rler, iki ayrÄ± Pinecone sunucusuz (serverless) indeksine (`rag-dense` ve `rag-sparse`) yÃ¼klenir.

### 2. Sorgulama (Inference) AkÄ±ÅŸÄ±

1.  **Girdi:** KullanÄ±cÄ±, Gradio arayÃ¼zÃ¼nden bir soru sorar.
2.  **Agent:** LangChain agent'Ä±, soruyu analiz eder ve `DocumentHybridSearch` aracÄ±nÄ± kullanmaya karar verir.
3.  **Hibrit Arama:**
    * Sorgu, hem dense (embedding) hem de sparse (TF-IDF) vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.
    * Her iki Pinecone indeksinde de `top_k` arama yapÄ±lÄ±r.
    * SonuÃ§larÄ±n skorlarÄ± normalleÅŸtirilir (`normalize_scores`) ve `alpha` deÄŸerine gÃ¶re birleÅŸtirilerek yeniden sÄ±ralanÄ±r.
4.  **YanÄ±t Ãœretme:** En iyi `top_k` sonuÃ§ (JSON formatÄ±nda) agent'a geri gÃ¶nderilir.
5.  **SonuÃ§:** Agent, bu arama sonuÃ§larÄ±nÄ± (context) kullanarak doÄŸal dilde bir yanÄ±t oluÅŸturur ve kullanÄ±cÄ±ya sunar.

## ğŸ› ï¸ Teknoloji Stack'i

* **LLM & Agent:** LangChain, Google Gemini (via `langchain-google-genai`), Transformers
* **VektÃ¶r VeritabanÄ±:** Pinecone (Serverless)
* **Embedding & VektÃ¶rleÅŸtirme:** SentenceTransformers, Scikit-learn (TfidfVectorizer)
* **ArayÃ¼z (UI):** Gradio
* **Veri Ä°ÅŸleme:** PyMuPDF (fitz), NLTK, Joblib
* **AltyapÄ± (Opsiyonel):** Docker (Pinecone local test servisleri iÃ§in)

## ğŸ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

### 1. Ã–n Gereksinimler

* Python 3.12
* Pinecone HesabÄ± (API Key iÃ§in)
* Google AI Studio HesabÄ± (Gemini API Key iÃ§in)
* Hugging Face HesabÄ± (Llama modelleri iÃ§in Token)

### 2. Kurulum

1.  **Projeyi klonlayÄ±n:**
    ```bash
    git clone [https://github.com/kullanici-adiniz/rag-llm.git](https://github.com/kullanici-adiniz/rag-llm.git)
    cd rag-llm
    ```

2.  **Sanal ortam oluÅŸturun ve aktifleÅŸtirin:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # (Windows iÃ§in: venv\Scripts\activate)
    ```

3.  **BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **NLTK Verisini Ä°ndirin:**
    `pipeline/chunking.py` dosyasÄ± NLTK'nÄ±n `punkt` modÃ¼lÃ¼nÃ¼ kullanÄ±yor. Ä°ndirmek iÃ§in:
    ```bash
    python -m nltk.downloader punkt_tab
    ```

5.  **`.env` DosyasÄ±nÄ± OluÅŸturun:**
    Proje ana dizininde `.env` adÄ±nda bir dosya oluÅŸturun ve `utils/config.py` dosyasÄ±na gÃ¶re aÅŸaÄŸÄ±daki deÄŸiÅŸkenleri doldurun:

    ```env
    PINECONE_API_KEY="YOUR_PINECONE_API_KEY"
    GOOGLE_API_KEY="YOUR_GOOGLE_GEMINI_API_KEY"

    # Llama 3.1 ve 3.2 modelleri 'gated' (eriÅŸim kÄ±sÄ±tlamalÄ±) modellerdir.
    # Bu modelleri kullanabilmek iÃ§in Hugging Face Hub token'Ä±nÄ±za ihtiyacÄ±nÄ±z olabilir.
    HUGGINGFACE_HUB_TOKEN="hf_YOUR_HUGGINGFACE_TOKEN"
    ```

### 3. Ã‡alÄ±ÅŸtÄ±rma

UygulamayÄ± baÅŸlatmak iÃ§in `app.py` dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
python app.py
```

## ğŸ“‚ Proje YapÄ±sÄ±

Projenin ana dizin yapÄ±sÄ± ve Ã¶nemli dosyalarÄ±n aÃ§Ä±klamalarÄ± aÅŸaÄŸÄ±dadÄ±r. `docs`, `saves` gibi klasÃ¶rler `config.py` iÃ§inde tanÄ±mlanmÄ±ÅŸtÄ±r ve uygulama Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda (`app.py`) otomatik olarak oluÅŸturulur.
```
rag-llm/
â”‚
â”œâ”€â”€ app.py                      # Gradio arayÃ¼zÃ¼nÃ¼ baÅŸlatan ve agent'Ä± yÃ¼kleyen ana uygulama dosyasÄ±
â”œâ”€â”€ docker-compose.yml          # (Opsiyonel) Pinecone local test servislerini baÅŸlatmak iÃ§in
â”œâ”€â”€ README.md                   # Bu dÃ¶kÃ¼man
â”‚
â”œâ”€â”€ pipeline/
â”‚ â”œâ”€â”€ init.py                   # Pipeline modÃ¼llerini import edilebilir hale getirir
â”‚ â”œâ”€â”€ chunking.py               # Anlamsal parÃ§alama (semantic chunking) mantÄ±ÄŸÄ±nÄ± iÃ§erir
â”‚ â””â”€â”€ contextualize.py          # LLM ile parÃ§alara baÄŸlam ekleme mantÄ±ÄŸÄ±nÄ± iÃ§erir
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ init.py                   # YardÄ±mcÄ± fonksiyonlarÄ± import edilebilir hale getirir
â”‚ â”œâ”€â”€ config.py                 # TÃ¼m konfigÃ¼rasyonlarÄ±, API anahtarlarÄ±nÄ± ve dosya yollarÄ±nÄ± yÃ¶netir
â”‚ â”œâ”€â”€ index_conf.py             # Pinecone indekslerini oluÅŸturma, silme ve sorgulama fonksiyonlarÄ±
â”‚ â”œâ”€â”€ index_manager.py          # TÃ¼m veri iÅŸleme (ingestion) pipelineâ€™Ä±nÄ± (run_rebuild) yÃ¶netir
â”‚ â”œâ”€â”€ io_utils.py               # JSONL dosyalarÄ±na yazma gibi I/O iÅŸlemleri
â”‚ â”œâ”€â”€ pdf_utils.py              # PDF dosyalarÄ±nÄ± ayrÄ±ÅŸtÄ±ran (parsing) fonksiyonlar
â”‚ â””â”€â”€ rag_core.py               # Hibrit arama (hybrid_search) ve skor normalleÅŸtirme mantÄ±ÄŸÄ±
â”‚
â”œâ”€â”€ docs/                       # (Dinamik) YÃ¼klenecek PDF'lerin konulduÄŸu klasÃ¶r
â”œâ”€â”€ processed_docs/             # (Dinamik) Ä°ÅŸlemi tamamlanan PDF'lerin taÅŸÄ±ndÄ±ÄŸÄ± klasÃ¶r
â”œâ”€â”€ saves/                      # (Dinamik) Ä°ÅŸleme sÄ±rasÄ±nda Ã¼retilen ara dosyalarÄ±n (chunks.jsonl, docs.jsonl vb.) kaydedildiÄŸi yer
â””â”€â”€ sparse_vectorizer/          # (Dinamik) EÄŸitilmiÅŸ TF-IDF modelinin (vectorizer.joblib) kaydedildiÄŸi yer
```
